{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import zeta\n",
    "from wordcloud import WordCloud\n",
    "nltk.download()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_words(path:str):\n",
    "    list_seed_words = []\n",
    "    with open(file=path, mode=\"r\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            list_seed_words.append(re.sub(\"\\n\", \"\", line))\n",
    "\n",
    "    return list_seed_words\n",
    "\n",
    "def extract_year(date: str):\n",
    "    return date[:4]\n",
    "\n",
    "def list_to_dict_count(lst:list):\n",
    "    out = nltk.defaultdict(int)\n",
    "    for token in lst:\n",
    "        out[token] += 1\n",
    "    return sorted(out.items(), key=lambda x : x[1], reverse=True)\n",
    "\n",
    "def top_n_not_stopwords(d: dict, n=10):\n",
    "    out = nltk.defaultdict(list)\n",
    "    stopwords_english = set(stopwords.words(\"english\"))\n",
    "    sorted_data = dict(sorted(d.items()))\n",
    "    for k, v in sorted_data.items():\n",
    "        for value in v:\n",
    "            if value[0] not in stopwords_english:\n",
    "                out[k].append(value)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\azizc\\AppData\\Local\\Temp\\ipykernel_11048\\4241532332.py:2: DtypeWarning: Columns (7,8,10,14,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_igbt = pd.read_csv(\"assign1_data/lgbt_news_corpus.csv\", encoding=\"ISO-8859-1\")\n"
     ]
    }
   ],
   "source": [
    "#load datasets\n",
    "df_igbt = pd.read_csv(\"assign1_data/lgbt_news_corpus.csv\", encoding=\"ISO-8859-1\")\n",
    "df_bg = pd.read_csv(\"assign1_data/background_news_corpus.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Text', 'Title', 'Id', 'Count', 'Date', 'Category', 'Unnamed: 6',\n",
       "       'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9', 'Unnamed: 10', 'Unnamed: 11',\n",
       "       'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14', 'Unnamed: 15',\n",
       "       'Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_igbt.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Text', 'Title', 'Id', 'Count', 'Date', 'Category'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bg.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove unnamed columns to be able to merge both datasets together later on\n",
    "df_igbt = df_igbt[[\"Text\", \"Title\", \"Id\", \"Count\", \"Date\", \"Category\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Text', 'Title', 'Id', 'Count', 'Date', 'Category'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_igbt.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge both datasets\n",
    "frames = [df_igbt, df_bg]\n",
    "merged_dataset = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Text', 'Title', 'Id', 'Count', 'Date', 'Category'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_dataset) == len(df_igbt) + len(df_bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the dataset\n",
    "# Add YEAR column from DATE \n",
    "merged_dataset[\"Year\"] = merged_dataset[\"Date\"].apply(lambda d: extract_year(d))\n",
    "\n",
    "#lowercase the text\n",
    "merged_dataset[\"Text_Lower\"] = merged_dataset[\"Text\"].apply(lambda t: str(t).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_words_list = seed_words(\"assign1_data\\seed_list.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context(dataF, seed_list: list, window_size=3):\n",
    "    text_data = dataF[\"Text_Lower\"].values\n",
    "    years = dataF[\"Year\"].values\n",
    "    word_per_years = nltk.defaultdict(list)\n",
    "    occurence = nltk.defaultdict(int)\n",
    "    seed_year_words_count_dict = nltk.defaultdict(dict)\n",
    "\n",
    "    for seed_w in seed_list:\n",
    "        for g_idx, sent in enumerate(text_data):\n",
    "            #preprocessing\n",
    "            sent_prep_0 = re.sub(\"\\'\\'\", \"\", sent)\n",
    "            sent_prep_1 = re.sub(\"``\", \"\", sent_prep_0)\n",
    "            sent_prep_2 = re.sub(\"\\'s\", \"\", sent_prep_1) \n",
    "            sent_prep_2 = re.sub(r\"''\", \"\", sent_prep_2)\n",
    "            sent_prep_2 = re.sub(r\"\\'\", \"\", sent_prep_2)\n",
    "            sent_prep_3 = re.sub(r'[^\\x00-\\x7F]+', '', sent_prep_2) #remove all non-ASCII characters\n",
    "            year = int(years[g_idx])\n",
    "\n",
    "            tokens = word_tokenize(sent_prep_3)\n",
    "            tokens_words = [w for w in tokens if w not in string.punctuation]\n",
    "            \n",
    "            for idx, t in enumerate(tokens_words):\n",
    "                #surroundings = []\n",
    "                if t == seed_w:\n",
    "                    left_words = tokens_words[max(0, idx-3):idx]\n",
    "                    right_words = tokens_words[idx+1:min(idx+4, len(tokens_words))]\n",
    "                    context_words = left_words + right_words\n",
    "                    for c_w in context_words:\n",
    "                        #occurence[c_w] += 1\n",
    "                        word_per_years[year].append(c_w)\n",
    "                    \n",
    "        seed_year_words_count_dict[seed_w] = word_per_years\n",
    "    #return occurence, sorted(occurence.items(), key= lambda x : x[1], reverse=True), word_per_years\n",
    "    return seed_year_words_count_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "contex_dict = context(dataF=merged_dataset, seed_list=seed_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contex_dict[\"gay\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLAM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
